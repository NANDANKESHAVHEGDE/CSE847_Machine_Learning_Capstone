{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45786208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADAPTIVE LOG DETRENDING MODEL\n",
    "==============================\n",
    "\n",
    "Key innovation: Different hotels need different settings!\n",
    "\n",
    "For EACH hotel, test combinations of:\n",
    "1. Detrending window: 7, 14, 21, 28 days\n",
    "2. Min training size: 180, 200, 250 days\n",
    "3. Feature sets: Simple (lags 1-5) vs Extended (all lags)\n",
    "\n",
    "Select best combination per hotel via CV.\n",
    "\n",
    "Goal: \n",
    "- Get 22+ hotels working (like baseline)\n",
    "- With better R² than baseline (0.1156)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "processed_data_path = Path('../data/full-data/processed')\n",
    "output_path = Path('../models/adaptive_log_method')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mapping_df = pd.read_csv('../data/full-data/hotel_mapping.csv')\n",
    "hotel_list = mapping_df['masked_id'].tolist()\n",
    "\n",
    "DATA_END_DATE = '2025-12-31'\n",
    "\n",
    "# XGBoost params (same as baseline)\n",
    "XGB_PARAMS = {\n",
    "    'max_depth': 4,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 2.0,\n",
    "    'min_child_weight': 5,\n",
    "    'gamma': 0.1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Configurations to try per hotel\n",
    "CONFIGURATIONS = []\n",
    "\n",
    "# Generate all combinations\n",
    "for trend_window in [7, 14, 21, 28]:\n",
    "    for min_train in [180, 200, 250]:\n",
    "        for feature_set in ['simple', 'extended']:\n",
    "            CONFIGURATIONS.append({\n",
    "                'trend_window': trend_window,\n",
    "                'min_train_days': min_train,\n",
    "                'test_window': 30,\n",
    "                'feature_set': feature_set,\n",
    "                'name': f'trend{trend_window}_train{min_train}_{feature_set}'\n",
    "            })\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADAPTIVE LOG DETRENDING MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Hotels to process: {len(hotel_list)}\")\n",
    "print(f\"Configurations per hotel: {len(CONFIGURATIONS)}\")\n",
    "print(f\"\\nWill test:\")\n",
    "print(f\"  - Detrending windows: 7, 14, 21, 28 days\")\n",
    "print(f\"  - Min training: 180, 200, 250 days\")\n",
    "print(f\"  - Feature sets: simple (lags 1-5), extended (all lags)\")\n",
    "print(f\"\\nGoal: Find best config for EACH hotel!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def detrend_log_series(series, window):\n",
    "    \"\"\"Detrend with FLEXIBLE window\"\"\"\n",
    "    trend = series.rolling(window=window, min_periods=1, center=False).mean()\n",
    "    detrended = series - trend\n",
    "    return detrended, trend\n",
    "\n",
    "def prepare_features(df, config):\n",
    "    \"\"\"\n",
    "    Prepare features with ADAPTIVE configuration\n",
    "    \n",
    "    config = {\n",
    "        'trend_window': 7/14/21/28,\n",
    "        'feature_set': 'simple' or 'extended'\n",
    "    }\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    feature_cols = []\n",
    "    \n",
    "    trend_window = config['trend_window']\n",
    "    feature_set = config['feature_set']\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPETITOR FEATURES\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Get lag columns based on feature set\n",
    "    all_comp_lag_cols = [col for col in df.columns \n",
    "                         if '_lag_' in col \n",
    "                         and 'base_rate' not in col.lower()\n",
    "                         and any(currency in col for currency in ['-USD', '-EUR', '-HKD', '-CNY'])]\n",
    "    \n",
    "    if len(all_comp_lag_cols) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Filter based on feature set\n",
    "    if feature_set == 'simple':\n",
    "        # Only lags 1-5 (like baseline)\n",
    "        comp_lag_cols = [col for col in all_comp_lag_cols \n",
    "                        if any(f'_lag_{i}' in col for i in [1, 2, 3, 4, 5])]\n",
    "    else:\n",
    "        # All available lags (extended)\n",
    "        comp_lag_cols = all_comp_lag_cols\n",
    "    \n",
    "    if len(comp_lag_cols) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Log + detrend with ADAPTIVE window\n",
    "    for col in comp_lag_cols:\n",
    "        log_prices = np.log(df_processed[col].replace(0, np.nan))\n",
    "        detrended, _ = detrend_log_series(log_prices, window=trend_window)\n",
    "        df_processed[f'{col}_log_detrended'] = detrended\n",
    "        feature_cols.append(f'{col}_log_detrended')\n",
    "    \n",
    "    # Market aggregates (only for extended)\n",
    "    if feature_set == 'extended':\n",
    "        lag1_cols = [col for col in comp_lag_cols if '_lag_1' in col]\n",
    "        \n",
    "        if len(lag1_cols) > 1:\n",
    "            for col in lag1_cols:\n",
    "                df_processed[f'{col}_log'] = np.log(df_processed[col].replace(0, np.nan))\n",
    "            \n",
    "            lag1_log_cols = [f'{col}_log' for col in lag1_cols]\n",
    "            \n",
    "            df_processed['comp_mean_log'] = df_processed[lag1_log_cols].mean(axis=1)\n",
    "            df_processed['comp_min_log'] = df_processed[lag1_log_cols].min(axis=1)\n",
    "            df_processed['comp_max_log'] = df_processed[lag1_log_cols].max(axis=1)\n",
    "            df_processed['comp_std_log'] = df_processed[lag1_log_cols].std(axis=1)\n",
    "            \n",
    "            feature_cols.extend(['comp_mean_log', 'comp_min_log', 'comp_max_log', 'comp_std_log'])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEMPORAL FEATURES (10 baseline features)\n",
    "    # ========================================================================\n",
    "    \n",
    "    temporal_cols = ['day_of_week', 'month', 'is_weekend', 'day_of_year']\n",
    "    temporal_cols = [col for col in temporal_cols if col in df_processed.columns]\n",
    "    feature_cols.extend(temporal_cols)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    if 'day_of_week' in df_processed.columns:\n",
    "        df_processed['sin_day_of_week'] = np.sin(2 * np.pi * df_processed['day_of_week'] / 7)\n",
    "        df_processed['cos_day_of_week'] = np.cos(2 * np.pi * df_processed['day_of_week'] / 7)\n",
    "        feature_cols.extend(['sin_day_of_week', 'cos_day_of_week'])\n",
    "    \n",
    "    if 'month' in df_processed.columns:\n",
    "        df_processed['sin_month'] = np.sin(2 * np.pi * df_processed['month'] / 12)\n",
    "        df_processed['cos_month'] = np.cos(2 * np.pi * df_processed['month'] / 12)\n",
    "        feature_cols.extend(['sin_month', 'cos_month'])\n",
    "    \n",
    "    if 'day_of_year' in df_processed.columns:\n",
    "        df_processed['sin_day_of_year'] = np.sin(2 * np.pi * df_processed['day_of_year'] / 365)\n",
    "        df_processed['cos_day_of_year'] = np.cos(2 * np.pi * df_processed['day_of_year'] / 365)\n",
    "        feature_cols.extend(['sin_day_of_year', 'cos_day_of_year'])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TARGET (with ADAPTIVE detrending window)\n",
    "    # ========================================================================\n",
    "    \n",
    "    y_original = df_processed['base_rate']\n",
    "    y_log = np.log(y_original.replace(0, np.nan))\n",
    "    y_detrended, y_trend = detrend_log_series(y_log, window=trend_window)\n",
    "    \n",
    "    X = df_processed[feature_cols].copy()\n",
    "    \n",
    "    # Drop NaN\n",
    "    valid_idx = ~(X.isnull().any(axis=1) | y_detrended.isnull() | y_log.isnull())\n",
    "    X = X[valid_idx].reset_index(drop=True)\n",
    "    y_detrended = y_detrended[valid_idx].reset_index(drop=True)\n",
    "    y_trend = y_trend[valid_idx].reset_index(drop=True)\n",
    "    y_original = y_original[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    return X, y_detrended, y_trend, y_original, feature_cols\n",
    "\n",
    "# ============================================================================\n",
    "# CV AND EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def time_series_cv_splits(n_samples, min_train, test_window):\n",
    "    \"\"\"Time-series CV with ADAPTIVE split sizes\"\"\"\n",
    "    splits = []\n",
    "    train_end = min_train\n",
    "    \n",
    "    while train_end + test_window <= n_samples and len(splits) < 6:\n",
    "        splits.append({\n",
    "            'train_idx': list(range(0, train_end)),\n",
    "            'test_idx': list(range(train_end, min(train_end + test_window, n_samples)))\n",
    "        })\n",
    "        train_end += test_window\n",
    "    \n",
    "    if len(splits) < 3:\n",
    "        return None\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def evaluate_configuration(X_all, y_detrended, y_trend, y_original, config):\n",
    "    \"\"\"Evaluate one configuration with CV\"\"\"\n",
    "    \n",
    "    splits = time_series_cv_splits(len(X_all), config['min_train_days'], config['test_window'])\n",
    "    \n",
    "    if splits is None:\n",
    "        return None\n",
    "    \n",
    "    test_r2_scores = []\n",
    "    train_r2_scores = []\n",
    "    \n",
    "    for split in splits:\n",
    "        X_train = X_all.iloc[split['train_idx']]\n",
    "        X_test = X_all.iloc[split['test_idx']]\n",
    "        y_train = y_detrended.iloc[split['train_idx']]\n",
    "        y_test_detrended = y_detrended.iloc[split['test_idx']]\n",
    "        y_trend_train = y_trend.iloc[split['train_idx']]\n",
    "        y_trend_test = y_trend.iloc[split['test_idx']]\n",
    "        y_train_original = y_original.iloc[split['train_idx']]\n",
    "        y_test_original = y_original.iloc[split['test_idx']]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model = XGBRegressor(**XGB_PARAMS)\n",
    "        model.fit(X_train_scaled, y_train, verbose=False)\n",
    "        \n",
    "        # Train predictions\n",
    "        y_train_pred_detrended = model.predict(X_train_scaled)\n",
    "        y_train_pred_log = y_train_pred_detrended + y_trend_train.values\n",
    "        y_train_pred = np.exp(y_train_pred_log)\n",
    "        train_r2 = r2_score(y_train_original, y_train_pred)\n",
    "        \n",
    "        # Test predictions\n",
    "        y_test_pred_detrended = model.predict(X_test_scaled)\n",
    "        y_test_pred_log = y_test_pred_detrended + y_trend_test.values\n",
    "        y_test_pred = np.exp(y_test_pred_log)\n",
    "        test_r2 = r2_score(y_test_original, y_test_pred)\n",
    "        \n",
    "        test_r2_scores.append(test_r2)\n",
    "        train_r2_scores.append(train_r2)\n",
    "    \n",
    "    return {\n",
    "        'mean_test_r2': float(np.mean(test_r2_scores)),\n",
    "        'mean_train_r2': float(np.mean(train_r2_scores)),\n",
    "        'n_folds': len(splits)\n",
    "    }\n",
    "\n",
    "def train_final_model(X_all, y_detrended):\n",
    "    \"\"\"Train final model\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_all)\n",
    "    \n",
    "    model = XGBRegressor(**XGB_PARAMS)\n",
    "    model.fit(X_scaled, y_detrended, verbose=False)\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# ============================================================================\n",
    "# ADAPTIVE PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_hotel_adaptive(hotel_id):\n",
    "    \"\"\"\n",
    "    Process one hotel with ADAPTIVE approach:\n",
    "    1. Try all configurations\n",
    "    2. Select best based on test R²\n",
    "    3. Train final model with best config\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data_file = processed_data_path / f'{hotel_id}_lagged_dataset.csv'\n",
    "        \n",
    "        if not data_file.exists():\n",
    "            return {'hotel_id': hotel_id, 'status': 'missing_file'}\n",
    "        \n",
    "        df = pd.read_csv(data_file)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df[df['date'] <= DATA_END_DATE].copy()\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Need minimum data for any config\n",
    "        if len(df) < 180 + 30:  # Smallest min_train + test\n",
    "            return {'hotel_id': hotel_id, 'status': 'insufficient_data',\n",
    "                   'observations': len(df)}\n",
    "        \n",
    "        # Try all configurations\n",
    "        config_results = {}\n",
    "        \n",
    "        for config in CONFIGURATIONS:\n",
    "            # Prepare features\n",
    "            result = prepare_features(df, config)\n",
    "            \n",
    "            if result is None:\n",
    "                continue\n",
    "            \n",
    "            X, y_detrended, y_trend, y_original, feature_cols = result\n",
    "            \n",
    "            # Need enough data for this config\n",
    "            if len(X) < config['min_train_days'] + config['test_window']:\n",
    "                continue\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_result = evaluate_configuration(X, y_detrended, y_trend, y_original, config)\n",
    "            \n",
    "            if eval_result is not None:\n",
    "                config_results[config['name']] = {\n",
    "                    'config': config,\n",
    "                    'test_r2': eval_result['mean_test_r2'],\n",
    "                    'train_r2': eval_result['mean_train_r2'],\n",
    "                    'n_folds': eval_result['n_folds'],\n",
    "                    'n_features': len(feature_cols)\n",
    "                }\n",
    "        \n",
    "        if len(config_results) == 0:\n",
    "            return {'hotel_id': hotel_id, 'status': 'all_configs_failed'}\n",
    "        \n",
    "        # Select BEST configuration\n",
    "        best_config_name = max(config_results.keys(), \n",
    "                              key=lambda k: config_results[k]['test_r2'])\n",
    "        best_config = config_results[best_config_name]['config']\n",
    "        best_test_r2 = config_results[best_config_name]['test_r2']\n",
    "        \n",
    "        # Train final model with best config\n",
    "        result = prepare_features(df, best_config)\n",
    "        X, y_detrended, y_trend, y_original, feature_cols = result\n",
    "        \n",
    "        model, scaler = train_final_model(X, y_detrended)\n",
    "        \n",
    "        # Save model\n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'feature_cols': feature_cols,\n",
    "            'config': best_config,\n",
    "            'trend_window': best_config['trend_window'],\n",
    "            'y_trend_last': y_trend.iloc[-1],\n",
    "            'method': 'adaptive_log_detrending'\n",
    "        }\n",
    "        \n",
    "        model_file = output_path / f'{hotel_id}_model.pkl'\n",
    "        with open(model_file, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        return {\n",
    "            'hotel_id': hotel_id,\n",
    "            'status': 'success',\n",
    "            'best_config_name': best_config_name,\n",
    "            'trend_window': best_config['trend_window'],\n",
    "            'min_train_days': best_config['min_train_days'],\n",
    "            'feature_set': best_config['feature_set'],\n",
    "            'test_r2': best_test_r2,\n",
    "            'train_r2': config_results[best_config_name]['train_r2'],\n",
    "            'n_features': len(feature_cols),\n",
    "            'n_folds': config_results[best_config_name]['n_folds'],\n",
    "            'n_configs_tried': len(config_results),\n",
    "            'all_configs': {k: {'test_r2': v['test_r2'], 'n_features': v['n_features']} \n",
    "                           for k, v in config_results.items()}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'hotel_id': hotel_id, 'status': 'error', 'error': str(e)}\n",
    "\n",
    "# ============================================================================\n",
    "# RUN FOR ALL HOTELS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nProcessing hotels with ADAPTIVE configurations...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_results = {}\n",
    "successful = []\n",
    "\n",
    "for idx, hotel_id in enumerate(hotel_list, 1):\n",
    "    print(f\"[{idx}/{len(hotel_list)}] {hotel_id}...\", end=' ')\n",
    "    \n",
    "    result = process_hotel_adaptive(hotel_id)\n",
    "    all_results[hotel_id] = result\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        successful.append(hotel_id)\n",
    "        config_info = f\"trend={result['trend_window']}, {result['feature_set']}\"\n",
    "        print(f\"R²={result['test_r2']:.4f} ({config_info})\")\n",
    "    else:\n",
    "        print(f\"{result['status']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ADAPTIVE LOG DETRENDING - RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nTotal hotels: {len(hotel_list)}\")\n",
    "print(f\"Successful: {len(successful)}\")\n",
    "print(f\"Failed: {len(hotel_list) - len(successful)}\")\n",
    "\n",
    "if len(successful) > 0:\n",
    "    test_r2_values = [all_results[h]['test_r2'] for h in successful]\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE STATISTICS:\")\n",
    "    print(f\"  Mean Test R²: {np.mean(test_r2_values):.4f}\")\n",
    "    print(f\"  Median Test R²: {np.median(test_r2_values):.4f}\")\n",
    "    print(f\"  Min Test R²: {np.min(test_r2_values):.4f}\")\n",
    "    print(f\"  Max Test R²: {np.max(test_r2_values):.4f}\")\n",
    "    print(f\"  Std Test R²: {np.std(test_r2_values):.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARISON TO YOUR BASELINE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  YOUR BASELINE:    Mean = 0.1156, Median = 0.2236 (22 hotels)\")\n",
    "    print(f\"  ADAPTIVE METHOD:  Mean = {np.mean(test_r2_values):.4f}, Median = {np.median(test_r2_values):.4f} ({len(successful)} hotels)\")\n",
    "    \n",
    "    improvement_mean = ((np.mean(test_r2_values) - 0.1156) / 0.1156) * 100\n",
    "    improvement_median = ((np.median(test_r2_values) - 0.2236) / 0.2236) * 100\n",
    "    \n",
    "    print(f\"\\n  Mean improvement: {improvement_mean:+.1f}%\")\n",
    "    print(f\"  Median improvement: {improvement_median:+.1f}%\")\n",
    "    \n",
    "    # Configuration analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONFIGURATION ANALYSIS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Trend window distribution\n",
    "    trend_windows = {}\n",
    "    for h in successful:\n",
    "        tw = all_results[h]['trend_window']\n",
    "        trend_windows[tw] = trend_windows.get(tw, 0) + 1\n",
    "    \n",
    "    print(f\"\\nBest Trend Window Distribution:\")\n",
    "    for tw in sorted(trend_windows.keys()):\n",
    "        count = trend_windows[tw]\n",
    "        print(f\"  {tw}-day window: {count} hotels ({count/len(successful)*100:.1f}%)\")\n",
    "    \n",
    "    # Feature set distribution\n",
    "    feature_sets = {}\n",
    "    for h in successful:\n",
    "        fs = all_results[h]['feature_set']\n",
    "        feature_sets[fs] = feature_sets.get(fs, 0) + 1\n",
    "    \n",
    "    print(f\"\\nBest Feature Set Distribution:\")\n",
    "    for fs, count in feature_sets.items():\n",
    "        print(f\"  {fs}: {count} hotels ({count/len(successful)*100:.1f}%)\")\n",
    "    \n",
    "    # Performance tiers\n",
    "    good = sum(1 for r2 in test_r2_values if r2 > 0)\n",
    "    high = sum(1 for r2 in test_r2_values if r2 > 0.4)\n",
    "    medium = sum(1 for r2 in test_r2_values if 0.2 < r2 <= 0.4)\n",
    "    low = sum(1 for r2 in test_r2_values if 0 < r2 <= 0.2)\n",
    "    negative = sum(1 for r2 in test_r2_values if r2 <= 0)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE TIERS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  GOOD (R² > 0): {good} hotels ({good/len(successful)*100:.1f}%)\")\n",
    "    print(f\"  HIGH (R² > 0.4): {high} hotels ({high/len(successful)*100:.1f}%)\")\n",
    "    print(f\"  MEDIUM (0.2-0.4): {medium} hotels ({medium/len(successful)*100:.1f}%)\")\n",
    "    print(f\"  LOW (0-0.2): {low} hotels ({low/len(successful)*100:.1f}%)\")\n",
    "    print(f\"  NEGATIVE: {negative} hotels ({negative/len(successful)*100:.1f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_path / 'adaptive_summary.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    deployment_df = pd.DataFrame([\n",
    "        {\n",
    "            'hotel_id': h,\n",
    "            'test_r2': all_results[h]['test_r2'],\n",
    "            'train_r2': all_results[h]['train_r2'],\n",
    "            'trend_window': all_results[h]['trend_window'],\n",
    "            'feature_set': all_results[h]['feature_set'],\n",
    "            'n_features': all_results[h]['n_features'],\n",
    "            'n_folds': all_results[h]['n_folds'],\n",
    "            'n_configs_tried': all_results[h]['n_configs_tried'],\n",
    "            'model_file': f'{h}_model.pkl'\n",
    "        }\n",
    "        for h in successful\n",
    "    ])\n",
    "    deployment_df = deployment_df.sort_values('test_r2', ascending=False)\n",
    "    deployment_df.to_csv(output_path / 'deployment_adaptive.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n Models saved to: {output_path}/\")\n",
    "    print(f\" Summary: adaptive_summary.json\")\n",
    "    print(f\" Deployment: deployment_adaptive.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"KEY INSIGHT: Each hotel got its optimal configuration!\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
