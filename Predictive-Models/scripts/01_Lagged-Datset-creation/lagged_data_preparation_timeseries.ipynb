{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89767baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T04:07:56.932141Z",
     "start_time": "2025-10-04T04:07:43.862469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LAGGED DATA PREPARATION - ADVANCED IMPUTATION VERSION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LAGGED DATA PREPARATION - ADVANCED IMPUTATION VERSION\n",
    "======================================================\n",
    "This notebook creates lagged features using the improved competitor data\n",
    "from advanced imputation methods (Time-Series Decay).\n",
    "\n",
    "Outputs: lagged_predictive_dataset_advanced_imputation.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LAGGED DATA PREPARATION - ADVANCED IMPUTATION VERSION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65a33f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T04:08:00.258958Z",
     "start_time": "2025-10-04T04:07:56.938361Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 1: DATA LOADING\n",
      "================================================================================\n",
      "Loaded dataset shape: (365, 11)\n",
      "Date range: 2025-09-16 00:00:00 to 2026-09-15 00:00:00\n",
      "Using TIME-SERIES DECAY IMPUTATION competitor data\n",
      "\n",
      "================================================================================\n",
      "SECTION 2: DATA STRUCTURE VALIDATION\n",
      "================================================================================\n",
      "Dataset structure:\n",
      "   base_rate  base_rate_normalized  day_of_week  month  is_weekend       date  \\\n",
      "0      219.0               188.290            1      9           0 2025-09-16   \n",
      "1      249.0               243.145            2      9           0 2025-09-17   \n",
      "2      269.0               279.715            3      9           0 2025-09-18   \n",
      "3      209.0               170.005            4      9           0 2025-09-19   \n",
      "4      209.0               170.005            5      9           1 2025-09-20   \n",
      "\n",
      "   booking-us-aqua-pacific-monarch-USD  booking-us-castle-kamaole-sands-USD  \\\n",
      "0                                179.0                                308.0   \n",
      "1                                214.0                                308.0   \n",
      "2                                179.0                                318.0   \n",
      "3                                214.0                                318.0   \n",
      "4                                209.0                                318.0   \n",
      "\n",
      "   booking-us-courtyard-by-marriott-maui-kahului-airport-USD  \\\n",
      "0                                              479.0           \n",
      "1                                              479.0           \n",
      "2                                              479.0           \n",
      "3                                              429.0           \n",
      "4                                              429.0           \n",
      "\n",
      "   booking-us-kohea-kai-resort-maui-USD  booking-us-ohana-waikiki-malia-USD  \n",
      "0                                389.00                              208.86  \n",
      "1                                404.00                              218.86  \n",
      "2                                414.00                              228.86  \n",
      "3                                409.86                              248.86  \n",
      "4                                384.00                              218.86  \n",
      "\n",
      "Columns: ['base_rate', 'base_rate_normalized', 'day_of_week', 'month', 'is_weekend', 'date', 'booking-us-aqua-pacific-monarch-USD', 'booking-us-castle-kamaole-sands-USD', 'booking-us-courtyard-by-marriott-maui-kahului-airport-USD', 'booking-us-kohea-kai-resort-maui-USD', 'booking-us-ohana-waikiki-malia-USD']\n",
      "Data types:\n",
      "base_rate                                                           float64\n",
      "base_rate_normalized                                                float64\n",
      "day_of_week                                                           int64\n",
      "month                                                                 int64\n",
      "is_weekend                                                            int64\n",
      "date                                                         datetime64[ns]\n",
      "booking-us-aqua-pacific-monarch-USD                                 float64\n",
      "booking-us-castle-kamaole-sands-USD                                 float64\n",
      "booking-us-courtyard-by-marriott-maui-kahului-airport-USD           float64\n",
      "booking-us-kohea-kai-resort-maui-USD                                float64\n",
      "booking-us-ohana-waikiki-malia-USD                                  float64\n",
      "dtype: object\n",
      "Missing values:\n",
      "base_rate                                                    0\n",
      "base_rate_normalized                                         0\n",
      "day_of_week                                                  0\n",
      "month                                                        0\n",
      "is_weekend                                                   0\n",
      "date                                                         0\n",
      "booking-us-aqua-pacific-monarch-USD                          0\n",
      "booking-us-castle-kamaole-sands-USD                          0\n",
      "booking-us-courtyard-by-marriott-maui-kahului-airport-USD    0\n",
      "booking-us-kohea-kai-resort-maui-USD                         0\n",
      "booking-us-ohana-waikiki-malia-USD                           0\n",
      "dtype: int64\n",
      "\n",
      "Price columns identified: ['base_rate', 'base_rate_normalized', 'day_of_week', 'month', 'is_weekend', 'booking-us-aqua-pacific-monarch-USD', 'booking-us-castle-kamaole-sands-USD', 'booking-us-courtyard-by-marriott-maui-kahului-airport-USD', 'booking-us-kohea-kai-resort-maui-USD', 'booking-us-ohana-waikiki-malia-USD']\n",
      "Focal hotel price column: base_rate\n",
      "\n",
      "================================================================================\n",
      "SECTION 3: STATISTICAL LAG ANALYSIS\n",
      "================================================================================\n",
      "Creating comprehensive lags for analysis...\n",
      "Analyzing focal hotel column: base_rate\n",
      "\n",
      "Autocorrelation Analysis:\n",
      "Lag\tACF\tPACF\tSignificant\n",
      "1\t0.726\t0.728\tYes\n",
      "2\t0.661\t0.286\tYes\n",
      "3\t0.607\t0.136\tYes\n",
      "4\t0.572\t0.098\tYes\n",
      "5\t0.574\t0.149\tYes\n",
      "6\t0.513\t-0.027\tYes\n",
      "7\t0.483\t0.011\tYes\n",
      "\n",
      "Cross-correlation with competitors:\n",
      "base_rate_normalized... - Optimal lag: 1, Correlation: 0.734\n",
      "day_of_week... - Optimal lag: 13, Correlation: 0.063\n",
      "month... - Optimal lag: 1, Correlation: -0.205\n",
      "\n",
      "AIC-based lag selection:\n",
      "Optimal max lag (AIC): 11\n",
      "AIC: 2226.01\n",
      "Features: 110\n",
      "\n",
      "Recommended lags based on analysis (capped at 5 days): [1, 2, 3, 4, 5]\n",
      "\n",
      "================================================================================\n",
      "SECTION 4: FINAL LAGGED FEATURE CREATION\n",
      "================================================================================\n",
      "Creating final lagged features with selected lags: [1, 2, 3, 4, 5]\n",
      "Dataset shape after adding selected lags: (365, 61)\n",
      "Missing values after lagging: 150\n",
      "Final dataset shape after removing NaN: (360, 61)\n",
      "Data retention: 98.6%\n",
      "\n",
      "================================================================================\n",
      "SECTION 5: TEMPORAL FEATURE ENGINEERING\n",
      "================================================================================\n",
      "Dataset shape after adding temporal features: (360, 68)\n",
      "Temporal features added: ['sin_day_of_week', 'cos_day_of_week', 'sin_month', 'cos_month', 'sin_day_of_year', 'cos_day_of_year', 'is_weekend']\n",
      "\n",
      "================================================================================\n",
      "SECTION 6: FEATURE CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "Top 10 correlations with base_rate:\n",
      "base_rate_lag_1                              0.735461\n",
      "base_rate_normalized_lag_1                   0.735461\n",
      "base_rate_lag_2                              0.676338\n",
      "base_rate_normalized_lag_2                   0.676338\n",
      "base_rate_lag_3                              0.616823\n",
      "base_rate_normalized_lag_3                   0.616823\n",
      "booking-us-castle-kamaole-sands-USD_lag_1    0.592620\n",
      "base_rate_lag_5                              0.592612\n",
      "base_rate_normalized_lag_5                   0.592612\n",
      "base_rate_lag_4                              0.584082\n",
      "dtype: float64\n",
      "\n",
      "Focal hotel lag correlations:\n",
      "base_rate_lag_1               0.735461\n",
      "base_rate_normalized_lag_1    0.735461\n",
      "base_rate_lag_2               0.676338\n",
      "base_rate_normalized_lag_2    0.676338\n",
      "base_rate_lag_3               0.616823\n",
      "base_rate_normalized_lag_3    0.616823\n",
      "base_rate_lag_5               0.592612\n",
      "base_rate_normalized_lag_5    0.592612\n",
      "base_rate_lag_4               0.584082\n",
      "base_rate_normalized_lag_4    0.584082\n",
      "dtype: float64\n",
      "\n",
      "Top competitor lag correlations:\n",
      "booking-us-castle-kamaole-sands-USD_lag_1    0.592620\n",
      "booking-us-castle-kamaole-sands-USD_lag_2    0.573950\n",
      "booking-us-castle-kamaole-sands-USD_lag_3    0.552317\n",
      "booking-us-castle-kamaole-sands-USD_lag_4    0.530340\n",
      "booking-us-castle-kamaole-sands-USD_lag_5    0.500847\n",
      "dtype: float64\n",
      "\n",
      "Total lag features created: 50\n",
      "Lag features: ['base_rate_lag_1', 'base_rate_lag_2', 'base_rate_lag_3', 'base_rate_lag_4', 'base_rate_lag_5']... (showing first 5)\n",
      "\n",
      "================================================================================\n",
      "SECTION 7: FINAL DATASET SUMMARY\n",
      "================================================================================\n",
      "Final dataset summary:\n",
      "  Observations: 360\n",
      "  Total features: 67\n",
      "  Lag features: 50\n",
      "  Temporal features: 7\n",
      "  Original features: 10\n",
      "  Data lost to lagging: 5 rows (1.4%)\n",
      "\n",
      "================================================================================\n",
      "SECTION 8: SAVE PREPARED DATASET\n",
      "================================================================================\n",
      "Dataset saved successfully!\n",
      "\n",
      "Files created:\n",
      "  - lagged_predictive_dataset_timeseries.csv\n",
      "  - lag_selection_metadata_timeseries.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_preprocessed_data_timeseries():\n",
    "    \"\"\"Load cleaned data using TIME-SERIES DECAY IMPUTATION competitor data\"\"\"\n",
    "    \n",
    "    data_path = Path().cwd().parent.parent.parent / 'data' / 'dataprocessed'\n",
    "    sys.path.append(str(data_path))\n",
    "    \n",
    "    # Use focal data (unchanged)\n",
    "    focal_daily = pd.read_csv(data_path / 'focal_daily_aggregated.csv')\n",
    "    \n",
    "    # Use TIME-SERIES DECAY IMPUTATION competitor matrix\n",
    "    competitor_matrix = pd.read_csv(data_path / 'competitor_price_matrix_timeseries.csv')\n",
    "    \n",
    "    focal_daily['date'] = pd.to_datetime(focal_daily['stay_date'])\n",
    "    competitor_matrix['date'] = pd.to_datetime(competitor_matrix['stay_date'])\n",
    "    \n",
    "    df_merged = focal_daily.merge(competitor_matrix, on='date', how='inner')\n",
    "    df_merged = df_merged.drop(['stay_date_x', 'stay_date_y'], axis=1, errors='ignore')\n",
    "    df_merged = df_merged.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "df_final = load_preprocessed_data_timeseries()\n",
    "print(f\"Loaded dataset shape: {df_final.shape}\")\n",
    "print(f\"Date range: {df_final['date'].min()} to {df_final['date'].max()}\")\n",
    "print(f\"Using TIME-SERIES DECAY IMPUTATION competitor data\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: DATA STRUCTURE VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: DATA STRUCTURE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(df_final.head())\n",
    "print(f\"\\nColumns: {list(df_final.columns)}\")\n",
    "print(f\"Data types:\\n{df_final.dtypes}\")\n",
    "print(f\"Missing values:\\n{df_final.isnull().sum()}\")\n",
    "\n",
    "price_columns = [col for col in df_final.columns if col != 'date']\n",
    "print(f\"\\nPrice columns identified: {price_columns}\")\n",
    "\n",
    "focal_col = None\n",
    "for col in price_columns:\n",
    "    if 'focal' in col.lower() or any(keyword in col.lower() for keyword in ['base_rate', 'price']):\n",
    "        focal_col = col\n",
    "        break\n",
    "\n",
    "if focal_col:\n",
    "    print(f\"Focal hotel price column: {focal_col}\")\n",
    "else:\n",
    "    print(\"Warning: Could not identify focal hotel price column\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: STATISTICAL LAG ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: STATISTICAL LAG ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_comprehensive_lags(df, price_columns, max_lag=14):\n",
    "    \"\"\"Create lags up to max_lag for analysis\"\"\"\n",
    "    lag_dfs = []\n",
    "    \n",
    "    for col in price_columns:\n",
    "        col_lags = {}\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            col_lags[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "        lag_dfs.append(pd.DataFrame(col_lags))\n",
    "    \n",
    "    df_analysis = pd.concat([df] + lag_dfs, axis=1)\n",
    "    return df_analysis\n",
    "\n",
    "def analyze_autocorrelation(series, max_lag=14):\n",
    "    \"\"\"Compute autocorrelation function\"\"\"\n",
    "    from statsmodels.tsa.stattools import acf, pacf\n",
    "    \n",
    "    autocorr = acf(series.dropna(), nlags=max_lag, fft=False)\n",
    "    partial_autocorr = pacf(series.dropna(), nlags=max_lag)\n",
    "    \n",
    "    return autocorr, partial_autocorr\n",
    "\n",
    "def analyze_cross_correlation(focal_series, competitor_series, max_lag=14):\n",
    "    \"\"\"Compute cross-correlation between focal and competitor prices\"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for lag in range(1, max_lag + 1):\n",
    "        competitor_lagged = competitor_series.shift(lag)\n",
    "        corr = focal_series.corr(competitor_lagged)\n",
    "        correlations[lag] = corr\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def select_optimal_lags_aic(df, focal_col, max_lag=14):\n",
    "    \"\"\"Select optimal lag structure using AIC\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    competitor_cols = [col for col in df.columns if col != focal_col and col != 'date' and 'lag' not in col]\n",
    "    \n",
    "    for max_lag_test in range(1, max_lag + 1):\n",
    "        lag_features = []\n",
    "        for col in [focal_col] + competitor_cols:\n",
    "            for lag in range(1, max_lag_test + 1):\n",
    "                if f'{col}_lag_{lag}' in df.columns:\n",
    "                    lag_features.append(f'{col}_lag_{lag}')\n",
    "        \n",
    "        if len(lag_features) > 0:\n",
    "            df_test = df[lag_features + [focal_col]].dropna()\n",
    "            \n",
    "            if len(df_test) > len(lag_features) + 1:\n",
    "                X = df_test[lag_features]\n",
    "                y = df_test[focal_col]\n",
    "                \n",
    "                model = LinearRegression().fit(X, y)\n",
    "                y_pred = model.predict(X)\n",
    "                mse = mean_squared_error(y, y_pred)\n",
    "                \n",
    "                n = len(y)\n",
    "                k = len(lag_features)\n",
    "                aic = n * np.log(mse) + 2 * k\n",
    "                \n",
    "                results[max_lag_test] = {\n",
    "                    'aic': aic,\n",
    "                    'mse': mse,\n",
    "                    'n_features': k,\n",
    "                    'n_obs': n\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Creating comprehensive lags for analysis...\")\n",
    "max_lag_analysis = 14\n",
    "price_columns = [col for col in df_final.columns if col != 'date']\n",
    "df_analysis = create_comprehensive_lags(df_final, price_columns, max_lag_analysis)\n",
    "\n",
    "if focal_col:\n",
    "    print(f\"Analyzing focal hotel column: {focal_col}\")\n",
    "    \n",
    "    autocorr, pacf_vals = analyze_autocorrelation(df_final[focal_col], max_lag_analysis)\n",
    "    \n",
    "    print(\"\\nAutocorrelation Analysis:\")\n",
    "    print(\"Lag\\tACF\\tPACF\\tSignificant\")\n",
    "    for i in range(1, min(8, len(autocorr))):\n",
    "        significant = \"Yes\" if abs(autocorr[i]) > 0.1 else \"No\"\n",
    "        print(f\"{i}\\t{autocorr[i]:.3f}\\t{pacf_vals[i]:.3f}\\t{significant}\")\n",
    "    \n",
    "    print(\"\\nCross-correlation with competitors:\")\n",
    "    competitor_cols = [col for col in price_columns if col != focal_col]\n",
    "    \n",
    "    optimal_lags_by_competitor = {}\n",
    "    for comp_col in competitor_cols[:3]:\n",
    "        cross_corr = analyze_cross_correlation(df_final[focal_col], df_final[comp_col], max_lag_analysis)\n",
    "        \n",
    "        max_corr_lag = max(cross_corr.keys(), key=lambda k: abs(cross_corr[k]))\n",
    "        optimal_lags_by_competitor[comp_col] = {\n",
    "            'optimal_lag': max_corr_lag,\n",
    "            'correlation': cross_corr[max_corr_lag]\n",
    "        }\n",
    "        \n",
    "        print(f\"{comp_col[:20]}... - Optimal lag: {max_corr_lag}, Correlation: {cross_corr[max_corr_lag]:.3f}\")\n",
    "    \n",
    "    print(\"\\nAIC-based lag selection:\")\n",
    "    aic_results = select_optimal_lags_aic(df_analysis, focal_col, max_lag_analysis)\n",
    "    \n",
    "    if aic_results:\n",
    "        optimal_lag_aic = min(aic_results.keys(), key=lambda k: aic_results[k]['aic'])\n",
    "        print(f\"Optimal max lag (AIC): {optimal_lag_aic}\")\n",
    "        print(f\"AIC: {aic_results[optimal_lag_aic]['aic']:.2f}\")\n",
    "        print(f\"Features: {aic_results[optimal_lag_aic]['n_features']}\")\n",
    "        \n",
    "        most_common_lags = []\n",
    "        for comp_data in optimal_lags_by_competitor.values():\n",
    "            if abs(comp_data['correlation']) > 0.05:\n",
    "                most_common_lags.append(comp_data['optimal_lag'])\n",
    "        \n",
    "        if most_common_lags:\n",
    "            from collections import Counter\n",
    "            lag_counts = Counter(most_common_lags)\n",
    "            significant_autocorr_lags = [i for i in range(1, 8) if abs(autocorr[i]) > 0.1]\n",
    "            \n",
    "            final_lags = list(set([1] + significant_autocorr_lags + [lag for lag, count in lag_counts.most_common(2)]))\n",
    "            final_lags = sorted([lag for lag in final_lags if lag <= min(optimal_lag_aic, 5)])\n",
    "            \n",
    "            print(f\"\\nRecommended lags based on analysis (capped at 5 days): {final_lags}\")\n",
    "        else:\n",
    "            final_lags = [1, 3, 7]\n",
    "            print(f\"\\nDefaulting to standard lags: {final_lags}\")\n",
    "    else:\n",
    "        final_lags = [1, 3, 7]\n",
    "        print(f\"\\nDefaulting to standard lags: {final_lags}\")\n",
    "else:\n",
    "    final_lags = [1, 3, 7]\n",
    "    print(\"Could not identify focal hotel column, using default lags: [1, 3, 7]\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: FINAL LAGGED FEATURE CREATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: FINAL LAGGED FEATURE CREATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_final_lagged_features(df, price_columns, selected_lags):\n",
    "    \"\"\"Create final lagged features based on analysis\"\"\"\n",
    "    lag_dfs = []\n",
    "    \n",
    "    for col in price_columns:\n",
    "        col_lags = {}\n",
    "        for lag in selected_lags:\n",
    "            col_lags[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "        lag_dfs.append(pd.DataFrame(col_lags))\n",
    "    \n",
    "    df_lagged = pd.concat([df] + lag_dfs, axis=1)\n",
    "    return df_lagged\n",
    "\n",
    "print(f\"Creating final lagged features with selected lags: {final_lags}\")\n",
    "df_with_lags = create_final_lagged_features(df_final, price_columns, final_lags)\n",
    "\n",
    "print(f\"Dataset shape after adding selected lags: {df_with_lags.shape}\")\n",
    "print(f\"Missing values after lagging: {df_with_lags.isnull().sum().sum()}\")\n",
    "\n",
    "df_with_lags_clean = df_with_lags.dropna()\n",
    "print(f\"Final dataset shape after removing NaN: {df_with_lags_clean.shape}\")\n",
    "print(f\"Data retention: {len(df_with_lags_clean)/len(df_final)*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: ADD TEMPORAL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"Add cyclical temporal features\"\"\"\n",
    "    df_temporal = df.copy()\n",
    "    \n",
    "    df_temporal['day_of_week'] = df_temporal['date'].dt.dayofweek\n",
    "    df_temporal['month'] = df_temporal['date'].dt.month\n",
    "    df_temporal['day_of_year'] = df_temporal['date'].dt.dayofyear\n",
    "    \n",
    "    df_temporal['sin_day_of_week'] = np.sin(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['cos_day_of_week'] = np.cos(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['sin_month'] = np.sin(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['cos_month'] = np.cos(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['sin_day_of_year'] = np.sin(2 * np.pi * df_temporal['day_of_year'] / 365)\n",
    "    df_temporal['cos_day_of_year'] = np.cos(2 * np.pi * df_temporal['day_of_year'] / 365)\n",
    "    \n",
    "    df_temporal['is_weekend'] = (df_temporal['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    return df_temporal\n",
    "\n",
    "df_with_temporal = add_temporal_features(df_with_lags_clean)\n",
    "print(f\"Dataset shape after adding temporal features: {df_with_temporal.shape}\")\n",
    "\n",
    "temporal_features = ['sin_day_of_week', 'cos_day_of_week', 'sin_month', 'cos_month', \n",
    "                    'sin_day_of_year', 'cos_day_of_year', 'is_weekend']\n",
    "print(f\"Temporal features added: {temporal_features}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: FEATURE CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: FEATURE CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_selected_lag_features(df, focal_col, selected_lags):\n",
    "    \"\"\"Analyze the performance of selected lag features\"\"\"\n",
    "    lag_columns = []\n",
    "    for col in df.columns:\n",
    "        if any(f'_lag_{lag}' in col for lag in selected_lags):\n",
    "            lag_columns.append(col)\n",
    "    \n",
    "    if focal_col and lag_columns:\n",
    "        correlations = df[lag_columns].corrwith(df[focal_col]).abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"Top 10 correlations with {focal_col}:\")\n",
    "        print(correlations.head(10))\n",
    "        \n",
    "        focal_lag_cols = [col for col in lag_columns if focal_col.split('_')[0] in col]\n",
    "        competitor_lag_cols = [col for col in lag_columns if col not in focal_lag_cols]\n",
    "        \n",
    "        if focal_lag_cols:\n",
    "            print(f\"\\nFocal hotel lag correlations:\")\n",
    "            focal_corrs = df[focal_lag_cols].corrwith(df[focal_col]).abs().sort_values(ascending=False)\n",
    "            print(focal_corrs)\n",
    "        \n",
    "        if competitor_lag_cols:\n",
    "            print(f\"\\nTop competitor lag correlations:\")\n",
    "            comp_corrs = df[competitor_lag_cols].corrwith(df[focal_col]).abs().sort_values(ascending=False)\n",
    "            print(comp_corrs.head(5))\n",
    "    \n",
    "    return lag_columns\n",
    "\n",
    "if focal_col:\n",
    "    final_lag_columns = analyze_selected_lag_features(df_with_temporal, focal_col, final_lags)\n",
    "    print(f\"\\nTotal lag features created: {len(final_lag_columns)}\")\n",
    "    print(f\"Lag features: {final_lag_columns[:5]}... (showing first 5)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7: FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_observations = len(df_with_temporal)\n",
    "total_features = len(df_with_temporal.columns) - 1\n",
    "lag_features = len([col for col in df_with_temporal.columns if 'lag' in col])\n",
    "temporal_features_count = len(temporal_features)\n",
    "\n",
    "print(f\"Final dataset summary:\")\n",
    "print(f\"  Observations: {total_observations}\")\n",
    "print(f\"  Total features: {total_features}\")\n",
    "print(f\"  Lag features: {lag_features}\")\n",
    "print(f\"  Temporal features: {temporal_features_count}\")\n",
    "print(f\"  Original features: {total_features - lag_features - temporal_features_count}\")\n",
    "print(f\"  Data lost to lagging: {len(df_final) - total_observations} rows ({(len(df_final) - total_observations)/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: SAVE DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 8: SAVE PREPARED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_path = Path().cwd().parent.parent / 'data' / 'dataprocessed'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save with timeseries suffix\n",
    "df_with_temporal.to_csv(output_path / 'lagged_predictive_dataset_timeseries.csv', index=False)\n",
    "\n",
    "lag_metadata = {\n",
    "    'imputation_method': 'timeseries',\n",
    "    'selected_lags': final_lags,\n",
    "    'lag_selection_method': 'statistical_analysis',\n",
    "    'focal_column': focal_col,\n",
    "    'total_lag_features': lag_features,\n",
    "    'temporal_features': temporal_features,\n",
    "    'final_observations': total_observations,\n",
    "    'data_retention_pct': round(len(df_with_temporal)/len(df_final)*100, 1),\n",
    "    'feature_summary': {\n",
    "        'total_features': total_features,\n",
    "        'lag_features': lag_features,\n",
    "        'temporal_features': temporal_features_count,\n",
    "        'original_features': total_features - lag_features - temporal_features_count\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'imputation_correlation_distortion': 0.0039,\n",
    "        'imputation_method': 'Time-Series Decay'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_path / 'lag_selection_metadata_timeseries.json', 'w') as f:\n",
    "    json.dump(lag_metadata, f, indent=2)\n",
    "\n",
    "print(\"Dataset saved successfully!\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - lagged_predictive_dataset_timeseries.csv\")\n",
    "print(f\"  - lag_selection_metadata_timeseries.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
