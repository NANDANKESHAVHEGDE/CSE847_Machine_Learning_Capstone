{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7917d237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:32.149547Z",
     "start_time": "2025-10-04T03:15:29.396581Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb0e33",
   "metadata": {},
   "source": [
    "## LOADING EXISTING PREPROCESSING NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1704eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:32.189300Z",
     "start_time": "2025-10-04T03:15:32.152558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (365, 11)\n",
      "Date range: 2025-09-16 00:00:00 to 2026-09-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\"Load cleaned data from 2SRI preprocessing pipeline\"\"\"\n",
    "    \n",
    "    data_path = Path().cwd().parent.parent.parent/ 'data' / 'dataprocessed'\n",
    "    sys.path.append(str(data_path))\n",
    "    \n",
    "    focal_daily = pd.read_csv(data_path / 'focal_daily_aggregated.csv')\n",
    "    competitor_matrix = pd.read_csv(data_path / 'competitor_price_matrix.csv')\n",
    "    \n",
    "    focal_daily['date'] = pd.to_datetime(focal_daily['stay_date'])\n",
    "    competitor_matrix['date'] = pd.to_datetime(competitor_matrix['stay_date'])\n",
    "    \n",
    "    df_merged = focal_daily.merge(competitor_matrix, on='date', how='inner')\n",
    "    df_merged = df_merged.drop(['stay_date_x', 'stay_date_y'], axis=1, errors='ignore')\n",
    "    df_merged = df_merged.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "df_final = load_preprocessed_data()\n",
    "print(f\"Loaded dataset shape: {df_final.shape}\")\n",
    "print(f\"Date range: {df_final['date'].min()} to {df_final['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ede454",
   "metadata": {},
   "source": [
    "## SECTION2: DATA STRUCTURE VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b820e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:32.213691Z",
     "start_time": "2025-10-04T03:15:32.193315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "   base_rate  base_rate_normalized  day_of_week  month  is_weekend       date  \\\n",
      "0      219.0               188.290            1      9           0 2025-09-16   \n",
      "1      249.0               243.145            2      9           0 2025-09-17   \n",
      "2      269.0               279.715            3      9           0 2025-09-18   \n",
      "3      209.0               170.005            4      9           0 2025-09-19   \n",
      "4      209.0               170.005            5      9           1 2025-09-20   \n",
      "\n",
      "   booking-us-aqua-pacific-monarch-USD  booking-us-castle-kamaole-sands-USD  \\\n",
      "0                                214.0                                308.0   \n",
      "1                                214.0                                308.0   \n",
      "2                                214.0                                318.0   \n",
      "3                                214.0                                318.0   \n",
      "4                                209.0                                318.0   \n",
      "\n",
      "   booking-us-courtyard-by-marriott-maui-kahului-airport-USD  \\\n",
      "0                                              479.0           \n",
      "1                                              479.0           \n",
      "2                                              479.0           \n",
      "3                                              429.0           \n",
      "4                                              429.0           \n",
      "\n",
      "   booking-us-kohea-kai-resort-maui-USD  booking-us-ohana-waikiki-malia-USD  \n",
      "0                                 389.0                              208.86  \n",
      "1                                 404.0                              218.86  \n",
      "2                                 404.0                              228.86  \n",
      "3                                 404.0                              248.86  \n",
      "4                                 384.0                              218.86  \n",
      "\n",
      "Columns: ['base_rate', 'base_rate_normalized', 'day_of_week', 'month', 'is_weekend', 'date', 'booking-us-aqua-pacific-monarch-USD', 'booking-us-castle-kamaole-sands-USD', 'booking-us-courtyard-by-marriott-maui-kahului-airport-USD', 'booking-us-kohea-kai-resort-maui-USD', 'booking-us-ohana-waikiki-malia-USD']\n",
      "Data types:\n",
      "base_rate                                                           float64\n",
      "base_rate_normalized                                                float64\n",
      "day_of_week                                                           int64\n",
      "month                                                                 int64\n",
      "is_weekend                                                            int64\n",
      "date                                                         datetime64[ns]\n",
      "booking-us-aqua-pacific-monarch-USD                                 float64\n",
      "booking-us-castle-kamaole-sands-USD                                 float64\n",
      "booking-us-courtyard-by-marriott-maui-kahului-airport-USD           float64\n",
      "booking-us-kohea-kai-resort-maui-USD                                float64\n",
      "booking-us-ohana-waikiki-malia-USD                                  float64\n",
      "dtype: object\n",
      "Missing values:\n",
      "base_rate                                                    0\n",
      "base_rate_normalized                                         0\n",
      "day_of_week                                                  0\n",
      "month                                                        0\n",
      "is_weekend                                                   0\n",
      "date                                                         0\n",
      "booking-us-aqua-pacific-monarch-USD                          0\n",
      "booking-us-castle-kamaole-sands-USD                          0\n",
      "booking-us-courtyard-by-marriott-maui-kahului-airport-USD    0\n",
      "booking-us-kohea-kai-resort-maui-USD                         0\n",
      "booking-us-ohana-waikiki-malia-USD                           0\n",
      "dtype: int64\n",
      "\n",
      "Price columns identified: ['base_rate', 'base_rate_normalized', 'day_of_week', 'month', 'is_weekend', 'booking-us-aqua-pacific-monarch-USD', 'booking-us-castle-kamaole-sands-USD', 'booking-us-courtyard-by-marriott-maui-kahului-airport-USD', 'booking-us-kohea-kai-resort-maui-USD', 'booking-us-ohana-waikiki-malia-USD']\n",
      "Focal hotel price column: base_rate\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset structure:\")\n",
    "print(df_final.head())\n",
    "print(f\"\\nColumns: {list(df_final.columns)}\")\n",
    "print(f\"Data types:\\n{df_final.dtypes}\")\n",
    "print(f\"Missing values:\\n{df_final.isnull().sum()}\")\n",
    "\n",
    "price_columns = [col for col in df_final.columns if col != 'date']\n",
    "print(f\"\\nPrice columns identified: {price_columns}\")\n",
    "\n",
    "focal_col = None\n",
    "for col in price_columns:\n",
    "    if 'focal' in col.lower() or any(keyword in col.lower() for keyword in ['base_rate', 'price']):\n",
    "        focal_col = col\n",
    "        break\n",
    "\n",
    "if focal_col:\n",
    "    print(f\"Focal hotel price column: {focal_col}\")\n",
    "else:\n",
    "    print(\"Warning: Could not identify focal hotel price column\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4072ec",
   "metadata": {},
   "source": [
    "## SECTION3:LAGGED FEATURE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63663a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:33.209432Z",
     "start_time": "2025-10-04T03:15:32.218720Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive lags for analysis...\n",
      "Analyzing focal hotel column: base_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_10012\\2062073370.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autocorrelation Analysis:\n",
      "Lag\tACF\tPACF\tSignificant\n",
      "1\t0.726\t0.728\tYes\n",
      "2\t0.661\t0.286\tYes\n",
      "3\t0.607\t0.136\tYes\n",
      "4\t0.572\t0.098\tYes\n",
      "5\t0.574\t0.149\tYes\n",
      "6\t0.513\t-0.027\tYes\n",
      "7\t0.483\t0.011\tYes\n",
      "\n",
      "Cross-correlation with competitors:\n",
      "base_rate_normalized... - Optimal lag: 1, Correlation: 0.734\n",
      "day_of_week... - Optimal lag: 13, Correlation: 0.063\n",
      "month... - Optimal lag: 1, Correlation: -0.205\n",
      "\n",
      "AIC-based lag selection:\n",
      "Optimal max lag (AIC): 13\n",
      "AIC: 2196.80\n",
      "Features: 130\n",
      "\n",
      "Recommended lags based on analysis (capped at 5 days): [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def create_comprehensive_lags(df, price_columns, max_lag=14):\n",
    "    \"\"\"Create lags up to max_lag for analysis\"\"\"\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    for col in price_columns:\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            df_analysis[f'{col}_lag_{lag}'] = df_analysis[col].shift(lag)\n",
    "    \n",
    "    return df_analysis\n",
    "\n",
    "def analyze_autocorrelation(series, max_lag=14):\n",
    "    \"\"\"Compute autocorrelation function\"\"\"\n",
    "    from statsmodels.tsa.stattools import acf, pacf\n",
    "    \n",
    "    autocorr = acf(series.dropna(), nlags=max_lag, fft=False)\n",
    "    partial_autocorr = pacf(series.dropna(), nlags=max_lag)\n",
    "    \n",
    "    return autocorr, partial_autocorr\n",
    "\n",
    "def analyze_cross_correlation(focal_series, competitor_series, max_lag=14):\n",
    "    \"\"\"Compute cross-correlation between focal and competitor prices\"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for lag in range(1, max_lag + 1):\n",
    "        competitor_lagged = competitor_series.shift(lag)\n",
    "        corr = focal_series.corr(competitor_lagged)\n",
    "        correlations[lag] = corr\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def select_optimal_lags_aic(df, focal_col, max_lag=14):\n",
    "    \"\"\"Select optimal lag structure using AIC\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "    \n",
    "    n_obs = len(df.dropna())\n",
    "    results = {}\n",
    "    \n",
    "    competitor_cols = [col for col in df.columns if col != focal_col and col != 'date' and 'lag' not in col]\n",
    "    \n",
    "    for max_lag_test in range(1, max_lag + 1):\n",
    "        lag_features = []\n",
    "        for col in [focal_col] + competitor_cols:\n",
    "            for lag in range(1, max_lag_test + 1):\n",
    "                if f'{col}_lag_{lag}' in df.columns:\n",
    "                    lag_features.append(f'{col}_lag_{lag}')\n",
    "        \n",
    "        if len(lag_features) > 0:\n",
    "            df_test = df[lag_features + [focal_col]].dropna()\n",
    "            \n",
    "            if len(df_test) > len(lag_features) + 1:\n",
    "                X = df_test[lag_features]\n",
    "                y = df_test[focal_col]\n",
    "                \n",
    "                model = LinearRegression().fit(X, y)\n",
    "                y_pred = model.predict(X)\n",
    "                mse = mean_squared_error(y, y_pred)\n",
    "                \n",
    "                n = len(y)\n",
    "                k = len(lag_features)\n",
    "                aic = n * np.log(mse) + 2 * k\n",
    "                \n",
    "                results[max_lag_test] = {\n",
    "                    'aic': aic,\n",
    "                    'mse': mse,\n",
    "                    'n_features': k,\n",
    "                    'n_obs': n\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Creating comprehensive lags for analysis...\")\n",
    "max_lag_analysis = 14\n",
    "price_columns = [col for col in df_final.columns if col != 'date']\n",
    "df_analysis = create_comprehensive_lags(df_final, price_columns, max_lag_analysis)\n",
    "\n",
    "focal_col = None\n",
    "for col in price_columns:\n",
    "    if 'focal' in col.lower() or any(keyword in col.lower() for keyword in ['base_rate', 'price']):\n",
    "        focal_col = col\n",
    "        break\n",
    "\n",
    "if focal_col:\n",
    "    print(f\"Analyzing focal hotel column: {focal_col}\")\n",
    "    \n",
    "    autocorr, pacf_vals = analyze_autocorrelation(df_final[focal_col], max_lag_analysis)\n",
    "    \n",
    "    print(\"\\nAutocorrelation Analysis:\")\n",
    "    print(\"Lag\\tACF\\tPACF\\tSignificant\")\n",
    "    for i in range(1, min(8, len(autocorr))):\n",
    "        significant = \"Yes\" if abs(autocorr[i]) > 0.1 else \"No\"\n",
    "        print(f\"{i}\\t{autocorr[i]:.3f}\\t{pacf_vals[i]:.3f}\\t{significant}\")\n",
    "    \n",
    "    print(\"\\nCross-correlation with competitors:\")\n",
    "    competitor_cols = [col for col in price_columns if col != focal_col]\n",
    "    \n",
    "    optimal_lags_by_competitor = {}\n",
    "    for comp_col in competitor_cols[:3]:\n",
    "        cross_corr = analyze_cross_correlation(df_final[focal_col], df_final[comp_col], max_lag_analysis)\n",
    "        \n",
    "        max_corr_lag = max(cross_corr.keys(), key=lambda k: abs(cross_corr[k]))\n",
    "        optimal_lags_by_competitor[comp_col] = {\n",
    "            'optimal_lag': max_corr_lag,\n",
    "            'correlation': cross_corr[max_corr_lag]\n",
    "        }\n",
    "        \n",
    "        print(f\"{comp_col[:20]}... - Optimal lag: {max_corr_lag}, Correlation: {cross_corr[max_corr_lag]:.3f}\")\n",
    "    \n",
    "    print(\"\\nAIC-based lag selection:\")\n",
    "    aic_results = select_optimal_lags_aic(df_analysis, focal_col, max_lag_analysis)\n",
    "    \n",
    "    if aic_results:\n",
    "        optimal_lag_aic = min(aic_results.keys(), key=lambda k: aic_results[k]['aic'])\n",
    "        print(f\"Optimal max lag (AIC): {optimal_lag_aic}\")\n",
    "        print(f\"AIC: {aic_results[optimal_lag_aic]['aic']:.2f}\")\n",
    "        print(f\"Features: {aic_results[optimal_lag_aic]['n_features']}\")\n",
    "        \n",
    "        most_common_lags = []\n",
    "        for comp_data in optimal_lags_by_competitor.values():\n",
    "            if abs(comp_data['correlation']) > 0.05:\n",
    "                most_common_lags.append(comp_data['optimal_lag'])\n",
    "        \n",
    "        if most_common_lags:\n",
    "            from collections import Counter\n",
    "            lag_counts = Counter(most_common_lags)\n",
    "            significant_autocorr_lags = [i for i in range(1, 8) if abs(autocorr[i]) > 0.1]\n",
    "            \n",
    "            final_lags = list(set([1] + significant_autocorr_lags + [lag for lag, count in lag_counts.most_common(2)]))\n",
    "            final_lags = sorted([lag for lag in final_lags if lag <= min(optimal_lag_aic,5)])  # Cap at 7 days\n",
    "            \n",
    "            print(f\"\\nRecommended lags based on analysis (capped at 5 days): {final_lags}\")\n",
    "        else:\n",
    "            final_lags = [1, 3, 7]\n",
    "            print(f\"\\nDefaulting to standard lags: {final_lags}\")\n",
    "    else:\n",
    "        final_lags = [1, 3, 7]\n",
    "        print(f\"\\nDefaulting to standard lags: {final_lags}\")\n",
    "else:\n",
    "    final_lags = [1, 3, 7]\n",
    "    print(\"Could not identify focal hotel column, using default lags: [1, 3, 7]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f18335",
   "metadata": {},
   "source": [
    "## FINAL LAGGED FEATURE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d77d4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:33.266241Z",
     "start_time": "2025-10-04T03:15:33.213437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating final lagged features with selected lags: [1, 2, 3, 4, 5]\n",
      "Dataset shape after adding selected lags: (365, 61)\n",
      "Missing values after lagging: 150\n",
      "Final dataset shape after removing NaN: (360, 61)\n",
      "Data retention: 98.6%\n"
     ]
    }
   ],
   "source": [
    "def create_final_lagged_features(df, price_columns, selected_lags):\n",
    "    \"\"\"Create final lagged features based on analysis\"\"\"\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    for col in price_columns:\n",
    "        for lag in selected_lags:\n",
    "            df_lagged[f'{col}_lag_{lag}'] = df_lagged[col].shift(lag)\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "print(f\"\\nCreating final lagged features with selected lags: {final_lags}\")\n",
    "df_with_lags = create_final_lagged_features(df_final, price_columns, final_lags)\n",
    "\n",
    "print(f\"Dataset shape after adding selected lags: {df_with_lags.shape}\")\n",
    "print(f\"Missing values after lagging: {df_with_lags.isnull().sum().sum()}\")\n",
    "\n",
    "df_with_lags_clean = df_with_lags.dropna()\n",
    "print(f\"Final dataset shape after removing NaN: {df_with_lags_clean.shape}\")\n",
    "print(f\"Data retention: {len(df_with_lags_clean)/len(df_final)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f259430",
   "metadata": {},
   "source": [
    "## SECTION5:EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b377bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:33.290623Z",
     "start_time": "2025-10-04T03:15:33.270250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after adding temporal features: (360, 68)\n",
      "Temporal features added: ['sin_day_of_week', 'cos_day_of_week', 'sin_month', 'cos_month', 'sin_day_of_year', 'cos_day_of_year', 'is_weekend']\n"
     ]
    }
   ],
   "source": [
    "def add_temporal_features(df):\n",
    "    \"\"\"Add cyclical temporal features\"\"\"\n",
    "    df_temporal = df.copy()\n",
    "    \n",
    "    df_temporal['day_of_week'] = df_temporal['date'].dt.dayofweek\n",
    "    df_temporal['month'] = df_temporal['date'].dt.month\n",
    "    df_temporal['day_of_year'] = df_temporal['date'].dt.dayofyear\n",
    "    \n",
    "    df_temporal['sin_day_of_week'] = np.sin(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['cos_day_of_week'] = np.cos(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "    df_temporal['sin_month'] = np.sin(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['cos_month'] = np.cos(2 * np.pi * df_temporal['month'] / 12)\n",
    "    df_temporal['sin_day_of_year'] = np.sin(2 * np.pi * df_temporal['day_of_year'] / 365)\n",
    "    df_temporal['cos_day_of_year'] = np.cos(2 * np.pi * df_temporal['day_of_year'] / 365)\n",
    "    \n",
    "    df_temporal['is_weekend'] = (df_temporal['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    return df_temporal\n",
    "\n",
    "df_with_temporal = add_temporal_features(df_with_lags_clean)\n",
    "print(f\"Dataset shape after adding temporal features: {df_with_temporal.shape}\")\n",
    "\n",
    "temporal_features = ['sin_day_of_week', 'cos_day_of_week', 'sin_month', 'cos_month', \n",
    "                    'sin_day_of_year', 'cos_day_of_year', 'is_weekend']\n",
    "print(f\"Temporal features added: {temporal_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e44ae9",
   "metadata": {},
   "source": [
    "## SECTION 6:EXPLORATORY ANALYSIS OF SELECTED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a8af4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:33.356271Z",
     "start_time": "2025-10-04T03:15:33.295140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 correlations with base_rate:\n",
      "base_rate_lag_1               0.735461\n",
      "base_rate_normalized_lag_1    0.735461\n",
      "base_rate_lag_2               0.676338\n",
      "base_rate_normalized_lag_2    0.676338\n",
      "base_rate_lag_3               0.616823\n",
      "base_rate_normalized_lag_3    0.616823\n",
      "base_rate_lag_5               0.592612\n",
      "base_rate_normalized_lag_5    0.592612\n",
      "base_rate_lag_4               0.584082\n",
      "base_rate_normalized_lag_4    0.584082\n",
      "dtype: float64\n",
      "\n",
      "Focal hotel lag correlations:\n",
      "base_rate_lag_1               0.735461\n",
      "base_rate_normalized_lag_1    0.735461\n",
      "base_rate_lag_2               0.676338\n",
      "base_rate_normalized_lag_2    0.676338\n",
      "base_rate_lag_3               0.616823\n",
      "base_rate_normalized_lag_3    0.616823\n",
      "base_rate_lag_5               0.592612\n",
      "base_rate_normalized_lag_5    0.592612\n",
      "base_rate_lag_4               0.584082\n",
      "base_rate_normalized_lag_4    0.584082\n",
      "dtype: float64\n",
      "\n",
      "Top competitor lag correlations:\n",
      "booking-us-kohea-kai-resort-maui-USD_lag_1    0.436698\n",
      "booking-us-castle-kamaole-sands-USD_lag_4     0.413683\n",
      "booking-us-castle-kamaole-sands-USD_lag_5     0.410530\n",
      "booking-us-castle-kamaole-sands-USD_lag_3     0.407917\n",
      "booking-us-castle-kamaole-sands-USD_lag_2     0.406174\n",
      "dtype: float64\n",
      "\n",
      "Total lag features created: 50\n",
      "Lag features: ['base_rate_lag_1', 'base_rate_lag_2', 'base_rate_lag_3', 'base_rate_lag_4', 'base_rate_lag_5']... (showing first 5)\n"
     ]
    }
   ],
   "source": [
    "def analyze_selected_lag_features(df, focal_col, selected_lags):\n",
    "    \"\"\"Analyze the performance of selected lag features\"\"\"\n",
    "    lag_columns = []\n",
    "    for col in df.columns:\n",
    "        if any(f'_lag_{lag}' in col for lag in selected_lags):\n",
    "            lag_columns.append(col)\n",
    "    \n",
    "    if focal_col and lag_columns:\n",
    "        correlations = df[lag_columns].corrwith(df[focal_col]).abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"Top 10 correlations with {focal_col}:\")\n",
    "        print(correlations.head(10))\n",
    "        \n",
    "        focal_lag_cols = [col for col in lag_columns if focal_col.split('_')[0] in col]\n",
    "        competitor_lag_cols = [col for col in lag_columns if col not in focal_lag_cols]\n",
    "        \n",
    "        if focal_lag_cols:\n",
    "            print(f\"\\nFocal hotel lag correlations:\")\n",
    "            focal_corrs = df[focal_lag_cols].corrwith(df[focal_col]).abs().sort_values(ascending=False)\n",
    "            print(focal_corrs)\n",
    "        \n",
    "        if competitor_lag_cols:\n",
    "            print(f\"\\nTop competitor lag correlations:\")\n",
    "            comp_corrs = df[competitor_lag_cols].corrwith(df[focal_col]).abs().sort_values(ascending=False)\n",
    "            print(comp_corrs.head(5))\n",
    "    \n",
    "    return lag_columns\n",
    "\n",
    "if focal_col:\n",
    "    final_lag_columns = analyze_selected_lag_features(df_with_temporal, focal_col, final_lags)\n",
    "    print(f\"\\nTotal lag features created: {len(final_lag_columns)}\")\n",
    "    print(f\"Lag features: {final_lag_columns[:5]}... (showing first 5)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d7feb",
   "metadata": {},
   "source": [
    "## SECTION7:SAVE PREPARED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0121db92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:33.367998Z",
     "start_time": "2025-10-04T03:15:33.359066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset summary:\n",
      "Observations: 360\n",
      "Total features: 67\n",
      "Lag features: 50\n",
      "Temporal features: 7\n",
      "Original features: 10\n",
      "Data lost to lagging: 5 rows (1.4%)\n"
     ]
    }
   ],
   "source": [
    "total_observations = len(df_with_temporal)\n",
    "total_features = len(df_with_temporal.columns) - 1\n",
    "lag_features = len([col for col in df_with_temporal.columns if 'lag' in col])\n",
    "temporal_features_count = len(temporal_features)\n",
    "\n",
    "print(f\"Final dataset summary:\")\n",
    "print(f\"Observations: {total_observations}\")\n",
    "print(f\"Total features: {total_features}\")\n",
    "print(f\"Lag features: {lag_features}\")\n",
    "print(f\"Temporal features: {temporal_features_count}\")\n",
    "print(f\"Original features: {total_features - lag_features - temporal_features_count}\")\n",
    "print(f\"Data lost to lagging: {len(df_final) - total_observations} rows ({(len(df_final) - total_observations)/len(df_final)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5003909",
   "metadata": {},
   "source": [
    "## SAVE PREPARED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de6d0b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T03:15:33.418795Z",
     "start_time": "2025-10-04T03:15:33.371429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully!\n",
      "Files created: lagged_predictive_dataset_baseline.csv, lag_selection_metadata_baseline.json\n",
      "Ready for in-sample predictive modeling\n"
     ]
    }
   ],
   "source": [
    "output_path = Path().cwd().parent.parent / 'data' / 'dataprocessed'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_with_temporal.to_csv(output_path / 'lagged_predictive_dataset_baseline.csv', index=False)\n",
    "\n",
    "lag_metadata = {\n",
    "    'selected_lags': final_lags,\n",
    "    'lag_selection_method': 'statistical_analysis',\n",
    "    'focal_column': focal_col,\n",
    "    'total_lag_features': lag_features,\n",
    "    'temporal_features': temporal_features,\n",
    "    'final_observations': total_observations,\n",
    "    'data_retention_pct': round(len(df_with_temporal)/len(df_final)*100, 1),\n",
    "    'feature_summary': {\n",
    "        'total_features': total_features,\n",
    "        'lag_features': lag_features,\n",
    "        'temporal_features': temporal_features_count,\n",
    "        'original_features': total_features - lag_features - temporal_features_count\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_path / 'lag_selection_metadata_baseline.json', 'w') as f:\n",
    "    json.dump(lag_metadata, f, indent=2)\n",
    "\n",
    "print(\"Dataset saved successfully!\")\n",
    "print(f\"Files created: lagged_predictive_dataset_baseline.csv, lag_selection_metadata_baseline.json\")\n",
    "print(f\"Ready for in-sample predictive modeling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
