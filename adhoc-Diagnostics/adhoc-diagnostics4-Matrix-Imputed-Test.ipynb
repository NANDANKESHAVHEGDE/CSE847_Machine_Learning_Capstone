{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306254e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T20:54:33.068855Z",
     "start_time": "2025-10-20T20:54:18.320321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE IMPUTATION QUALITY ANALYSIS\n",
      "================================================================================\n",
      "Analyzing ALL 41 hotels\n",
      "Comparing BEFORE and AFTER imputation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Analyzing all hotels...\n",
      "  Progress: 5/41 hotels...\n",
      "  ERROR analyzing Hotel_08: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_08_lagged_dataset.csv'\n",
      "  Progress: 10/41 hotels...\n",
      "  ERROR analyzing Hotel_11: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_11_lagged_dataset.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_2344\\2728321625.py\", line 48, in analyze_imputation_quality\n",
      "    df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ~~~~~~~~~~^\n",
      "        f,\n",
      "        ^^\n",
      "    ...<6 lines>...\n",
      "        storage_options=self.options.get(\"storage_options\", None),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "        handle,\n",
      "    ...<3 lines>...\n",
      "        newline=\"\",\n",
      "    )\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_08_lagged_dataset.csv'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_2344\\2728321625.py\", line 48, in analyze_imputation_quality\n",
      "    df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ~~~~~~~~~~^\n",
      "        f,\n",
      "        ^^\n",
      "    ...<6 lines>...\n",
      "        storage_options=self.options.get(\"storage_options\", None),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "        handle,\n",
      "    ...<3 lines>...\n",
      "        newline=\"\",\n",
      "    )\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_11_lagged_dataset.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 15/41 hotels...\n",
      "  ERROR analyzing Hotel_16: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_16_lagged_dataset.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_2344\\2728321625.py\", line 48, in analyze_imputation_quality\n",
      "    df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ~~~~~~~~~~^\n",
      "        f,\n",
      "        ^^\n",
      "    ...<6 lines>...\n",
      "        storage_options=self.options.get(\"storage_options\", None),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "        handle,\n",
      "    ...<3 lines>...\n",
      "        newline=\"\",\n",
      "    )\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_16_lagged_dataset.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 20/41 hotels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 25/41 hotels...\n",
      "  Progress: 30/41 hotels...\n",
      "  Progress: 35/41 hotels...\n",
      "  ERROR analyzing Hotel_42: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_42_lagged_dataset.csv'\n",
      "  Progress: 40/41 hotels...\n",
      "  ERROR analyzing Hotel_43: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_43_lagged_dataset.csv'\n",
      "  ERROR analyzing Hotel_44: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_44_lagged_dataset.csv'\n",
      "\n",
      "Completed analysis for 35 hotels\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE IMPUTATION QUALITY REPORT\n",
      "================================================================================\n",
      "\n",
      "1. FOCAL HOTEL PRICE ISSUES:\n",
      "----------------------------------------\n",
      "Hotel_13:\n",
      "  Negative: 0, Zero: 42\n",
      "  Price range: $0.00 - $289.00\n",
      "  Mean: $136.78, CV: 0.490\n",
      "Hotel_15:\n",
      "  Negative: 0, Zero: 310\n",
      "  Price range: $0.00 - $699.00\n",
      "  Mean: $119.60, CV: 1.871\n",
      "\n",
      "2. COMPETITOR PRICE ISSUES:\n",
      "----------------------------------------\n",
      "  ✓ No major issues found\n",
      "\n",
      "3. REMAINING MISSING DATA (>10% after imputation):\n",
      "----------------------------------------\n",
      "  ✓ All hotels have <10% missing data\n",
      "\n",
      "4. WEAK COMPETITOR CORRELATIONS:\n",
      "----------------------------------------\n",
      "Hotel_19: Mean corr = -0.074, Weak = 4/5\n",
      "Hotel_09: Mean corr = 0.064, Weak = 4/5\n",
      "Hotel_18: Mean corr = 0.217, Weak = 4/6\n",
      "Hotel_23: Mean corr = 0.251, Weak = 6/10\n",
      "Hotel_33: Mean corr = 0.257, Weak = 3/6\n",
      "Hotel_12: Mean corr = 0.265, Weak = 5/9\n",
      "\n",
      "5. IMPUTATION ARTIFACTS (High Duplication >30%):\n",
      "----------------------------------------\n",
      "Hotel_20:\n",
      "  Max identical: 100.0%\n",
      "  Competitors with >30% duplication: 3/8\n",
      "  Mean uniqueness: 25.5%\n",
      "Hotel_18:\n",
      "  Max identical: 86.7%\n",
      "  Competitors with >30% duplication: 4/6\n",
      "  Mean uniqueness: 19.6%\n",
      "Hotel_02:\n",
      "  Max identical: 79.8%\n",
      "  Competitors with >30% duplication: 1/4\n",
      "  Mean uniqueness: 19.7%\n",
      "Hotel_25:\n",
      "  Max identical: 56.5%\n",
      "  Competitors with >30% duplication: 1/10\n",
      "  Mean uniqueness: 36.0%\n",
      "Hotel_23:\n",
      "  Max identical: 47.6%\n",
      "  Competitors with >30% duplication: 1/10\n",
      "  Mean uniqueness: 31.1%\n",
      "Hotel_37:\n",
      "  Max identical: 40.2%\n",
      "  Competitors with >30% duplication: 1/5\n",
      "  Mean uniqueness: 35.0%\n",
      "Hotel_15:\n",
      "  Max identical: 36.8%\n",
      "  Competitors with >30% duplication: 1/5\n",
      "  Mean uniqueness: 33.0%\n",
      "Hotel_41:\n",
      "  Max identical: 32.5%\n",
      "  Competitors with >30% duplication: 1/6\n",
      "  Mean uniqueness: 25.2%\n",
      "\n",
      "6. TEMPORAL CONSISTENCY ISSUES:\n",
      "----------------------------------------\n",
      "Hotel_05:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 51 days\n",
      "Hotel_07:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 102 days\n",
      "Hotel_13:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 63 days\n",
      "Hotel_14:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 179 days\n",
      "Hotel_15:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 61 days\n",
      "Hotel_18:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 224 days\n",
      "Hotel_21:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 73 days\n",
      "Hotel_23:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 179 days\n",
      "Hotel_25:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 179 days\n",
      "Hotel_33:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 47 days\n",
      "Hotel_36:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 145 days\n",
      "Hotel_37:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 48 days\n",
      "Hotel_40:\n",
      "  Competitors with flat periods: 5\n",
      "  Max consecutive identical values: 122 days\n",
      "\n",
      "================================================================================\n",
      "OVERALL SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total hotels analyzed: 35\n",
      "\n",
      "Focal Hotel Prices:\n",
      "  Hotels with negative prices: 0\n",
      "  Hotels with zero prices: 2\n",
      "  Mean price CV: 0.358\n",
      "\n",
      "Competitor Data Quality:\n",
      "  Hotels with >10% missing data: 0\n",
      "  Mean missing data: 0.0%\n",
      "\n",
      "Correlations:\n",
      "  Hotels with mean corr <0.3: 6\n",
      "  Overall mean correlation: 0.466\n",
      "\n",
      "Imputation Artifacts:\n",
      "  Hotels with >30% duplication: 8\n",
      "  Mean max duplication: 27.1%\n",
      "\n",
      "\n",
      "Detailed results saved to: ..\\data\\full-data\\imputation_analysis\n",
      "Files:\n",
      "  - imputation_analysis_complete.json (full details)\n",
      "  - imputation_summary.csv (summary table)\n",
      "\n",
      "Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_2344\\2728321625.py\", line 48, in analyze_imputation_quality\n",
      "    df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ~~~~~~~~~~^\n",
      "        f,\n",
      "        ^^\n",
      "    ...<6 lines>...\n",
      "        storage_options=self.options.get(\"storage_options\", None),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "        handle,\n",
      "    ...<3 lines>...\n",
      "        newline=\"\",\n",
      "    )\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_42_lagged_dataset.csv'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_2344\\2728321625.py\", line 48, in analyze_imputation_quality\n",
      "    df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ~~~~~~~~~~^\n",
      "        f,\n",
      "        ^^\n",
      "    ...<6 lines>...\n",
      "        storage_options=self.options.get(\"storage_options\", None),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "        handle,\n",
      "    ...<3 lines>...\n",
      "        newline=\"\",\n",
      "    )\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_43_lagged_dataset.csv'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Local\\Temp\\ipykernel_2344\\2728321625.py\", line 48, in analyze_imputation_quality\n",
      "    df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ~~~~~~~~~~^\n",
      "        f,\n",
      "        ^^\n",
      "    ...<6 lines>...\n",
      "        storage_options=self.options.get(\"storage_options\", None),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Nandan Hegde\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "        handle,\n",
      "    ...<3 lines>...\n",
      "        newline=\"\",\n",
      "    )\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '..\\\\data\\\\full-data\\\\processed\\\\Hotel_44_lagged_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "data_path = Path('../data/full-data/processed')\n",
    "raw_data_path = Path('../data/full-data')  # Original data before imputation\n",
    "output_path = Path('../data/full-data/imputation_analysis')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mapping_df = pd.read_csv('../data/full-data/hotel_mapping.csv')\n",
    "hotel_list = mapping_df['masked_id'].tolist()\n",
    "\n",
    "CATASTROPHIC_HOTELS = ['Hotel_26', 'Hotel_32', 'Hotel_34']\n",
    "ALL_HOTELS = [h for h in hotel_list if h not in CATASTROPHIC_HOTELS]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE IMPUTATION QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Analyzing ALL {len(ALL_HOTELS)} hotels\")\n",
    "print(f\"Comparing BEFORE and AFTER imputation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def load_original_data(hotel_id):\n",
    "    \"\"\"\n",
    "    Try to load original data BEFORE imputation\n",
    "    This will help us see what was actually imputed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to find original query results or raw data\n",
    "        # Adjust path based on your data structure\n",
    "        original_file = raw_data_path / f'query_results/{hotel_id}_raw.csv'\n",
    "        if original_file.exists():\n",
    "            return pd.read_csv(original_file)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def analyze_imputation_quality(hotel_id):\n",
    "    \"\"\"\n",
    "    Deep analysis comparing original vs imputed data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load processed (imputed) data\n",
    "        df_imputed = pd.read_csv(data_path / f'{hotel_id}_lagged_dataset.csv')\n",
    "        df_imputed['date'] = pd.to_datetime(df_imputed['date'])\n",
    "        df_imputed = df_imputed.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        if 'base_rate' not in df_imputed.columns:\n",
    "            return None\n",
    "        \n",
    "        # Get competitor columns (all lags)\n",
    "        all_comp_cols = [col for col in df_imputed.columns if '-USD' in col and 'lag_' in col]\n",
    "        comp_lag1_cols = [col for col in all_comp_cols if 'lag_1' in col]\n",
    "        \n",
    "        analysis = {\n",
    "            'hotel_id': hotel_id,\n",
    "            'n_observations': len(df_imputed),\n",
    "            'n_competitors': len(comp_lag1_cols),\n",
    "            'n_competitor_features': len(all_comp_cols)\n",
    "        }\n",
    "        \n",
    "        # ============================================================\n",
    "        # 1. FOCAL HOTEL PRICE ANALYSIS\n",
    "        # ============================================================\n",
    "        focal_prices = df_imputed['base_rate']\n",
    "        analysis['focal_price_stats'] = {\n",
    "            'mean': float(focal_prices.mean()),\n",
    "            'median': float(focal_prices.median()),\n",
    "            'std': float(focal_prices.std()),\n",
    "            'min': float(focal_prices.min()),\n",
    "            'max': float(focal_prices.max()),\n",
    "            'q25': float(focal_prices.quantile(0.25)),\n",
    "            'q75': float(focal_prices.quantile(0.75)),\n",
    "            'n_missing': int(focal_prices.isna().sum()),\n",
    "            'pct_missing': float((focal_prices.isna().sum() / len(focal_prices)) * 100),\n",
    "            'n_zero': int((focal_prices == 0).sum()),\n",
    "            'n_negative': int((focal_prices < 0).sum()),\n",
    "            'cv': float(focal_prices.std() / focal_prices.mean()) if focal_prices.mean() > 0 else None\n",
    "        }\n",
    "        \n",
    "        # ============================================================\n",
    "        # 2. COMPETITOR PRICE ANALYSIS (ALL FEATURES)\n",
    "        # ============================================================\n",
    "        if len(all_comp_cols) > 0:\n",
    "            comp_data_all = df_imputed[all_comp_cols]\n",
    "            \n",
    "            # Missing data patterns BEFORE imputation (if we can detect it)\n",
    "            # We'll look at patterns that suggest imputation\n",
    "            analysis['competitor_all_features'] = {\n",
    "                'total_cells': int(comp_data_all.size),\n",
    "                'missing_cells': int(comp_data_all.isna().sum().sum()),\n",
    "                'missing_pct': float((comp_data_all.isna().sum().sum() / comp_data_all.size) * 100),\n",
    "                'features_with_missing': int((comp_data_all.isna().any()).sum()),\n",
    "                'features_all_missing': int((comp_data_all.isna().all()).sum())\n",
    "            }\n",
    "            \n",
    "            # Value distributions\n",
    "            flat_values = comp_data_all.values.flatten()\n",
    "            flat_values = flat_values[~np.isnan(flat_values)]\n",
    "            \n",
    "            if len(flat_values) > 0:\n",
    "                analysis['competitor_price_stats_all'] = {\n",
    "                    'mean': float(np.mean(flat_values)),\n",
    "                    'median': float(np.median(flat_values)),\n",
    "                    'std': float(np.std(flat_values)),\n",
    "                    'min': float(np.min(flat_values)),\n",
    "                    'max': float(np.max(flat_values)),\n",
    "                    'n_zero': int((flat_values == 0).sum()),\n",
    "                    'n_negative': int((flat_values < 0).sum()),\n",
    "                    'pct_zero': float((flat_values == 0).sum() / len(flat_values) * 100),\n",
    "                    'pct_negative': float((flat_values < 0).sum() / len(flat_values) * 100)\n",
    "                }\n",
    "        \n",
    "        # ============================================================\n",
    "        # 3. LAG 1 COMPETITOR ANALYSIS (for correlation)\n",
    "        # ============================================================\n",
    "        if len(comp_lag1_cols) > 0:\n",
    "            comp_data_lag1 = df_imputed[comp_lag1_cols]\n",
    "            \n",
    "            analysis['competitor_lag1'] = {\n",
    "                'missing_pct': float((comp_data_lag1.isna().sum().sum() / comp_data_lag1.size) * 100),\n",
    "                'competitors_with_missing': int((comp_data_lag1.isna().any()).sum())\n",
    "            }\n",
    "            \n",
    "            # ============================================================\n",
    "            # 4. CORRELATION ANALYSIS (Focal vs Competitors)\n",
    "            # ============================================================\n",
    "            correlations = []\n",
    "            for comp in comp_lag1_cols:\n",
    "                valid_mask = ~(focal_prices.isna() | df_imputed[comp].isna())\n",
    "                n_valid = valid_mask.sum()\n",
    "                \n",
    "                if n_valid > 10:\n",
    "                    corr = focal_prices[valid_mask].corr(df_imputed[comp][valid_mask])\n",
    "                    correlations.append({\n",
    "                        'competitor': comp,\n",
    "                        'correlation': float(corr) if not np.isnan(corr) else None,\n",
    "                        'n_valid_pairs': int(n_valid),\n",
    "                        'pct_valid': float((n_valid / len(df_imputed)) * 100)\n",
    "                    })\n",
    "            \n",
    "            if correlations:\n",
    "                valid_corrs = [c['correlation'] for c in correlations if c['correlation'] is not None]\n",
    "                if valid_corrs:\n",
    "                    analysis['correlations'] = {\n",
    "                        'mean_corr': float(np.mean(valid_corrs)),\n",
    "                        'median_corr': float(np.median(valid_corrs)),\n",
    "                        'min_corr': float(np.min(valid_corrs)),\n",
    "                        'max_corr': float(np.max(valid_corrs)),\n",
    "                        'std_corr': float(np.std(valid_corrs)),\n",
    "                        'n_negative_corr': int(sum(1 for c in valid_corrs if c < 0)),\n",
    "                        'n_weak_corr': int(sum(1 for c in valid_corrs if abs(c) < 0.3)),\n",
    "                        'all_correlations': correlations\n",
    "                    }\n",
    "            \n",
    "            # ============================================================\n",
    "            # 5. IMPUTATION ARTIFACTS DETECTION\n",
    "            # ============================================================\n",
    "            # Check for identical values (sign of poor imputation)\n",
    "            identical_analysis = []\n",
    "            for comp in comp_lag1_cols:\n",
    "                comp_values = df_imputed[comp].dropna()\n",
    "                if len(comp_values) > 0:\n",
    "                    value_counts = comp_values.value_counts()\n",
    "                    max_identical = value_counts.iloc[0]\n",
    "                    pct_identical = (max_identical / len(comp_values)) * 100\n",
    "                    \n",
    "                    # Check for suspiciously repeated values\n",
    "                    top_5_values = value_counts.head(5)\n",
    "                    \n",
    "                    identical_analysis.append({\n",
    "                        'competitor': comp,\n",
    "                        'max_identical_value': float(comp_values.mode()[0]) if len(comp_values.mode()) > 0 else None,\n",
    "                        'max_identical_count': int(max_identical),\n",
    "                        'max_identical_pct': float(pct_identical),\n",
    "                        'unique_values': int(len(value_counts)),\n",
    "                        'unique_pct': float((len(value_counts) / len(comp_values)) * 100),\n",
    "                        'top_5_concentration': float((top_5_values.sum() / len(comp_values)) * 100)\n",
    "                    })\n",
    "            \n",
    "            if identical_analysis:\n",
    "                analysis['imputation_artifacts'] = {\n",
    "                    'max_pct_identical': float(max([ia['max_identical_pct'] for ia in identical_analysis])),\n",
    "                    'competitors_high_duplication_50': int(sum(1 for ia in identical_analysis if ia['max_identical_pct'] > 50)),\n",
    "                    'competitors_high_duplication_30': int(sum(1 for ia in identical_analysis if ia['max_identical_pct'] > 30)),\n",
    "                    'competitors_low_uniqueness': int(sum(1 for ia in identical_analysis if ia['unique_pct'] < 10)),\n",
    "                    'mean_uniqueness_pct': float(np.mean([ia['unique_pct'] for ia in identical_analysis])),\n",
    "                    'details': identical_analysis\n",
    "                }\n",
    "            \n",
    "            # ============================================================\n",
    "            # 6. PRICE RATIO ANALYSIS (Focal vs Competitors)\n",
    "            # ============================================================\n",
    "            price_ratios = []\n",
    "            for comp in comp_lag1_cols:\n",
    "                valid_mask = (focal_prices > 0) & (df_imputed[comp] > 0)\n",
    "                if valid_mask.sum() > 10:\n",
    "                    ratio = (focal_prices[valid_mask] / df_imputed[comp][valid_mask])\n",
    "                    price_ratios.append({\n",
    "                        'competitor': comp,\n",
    "                        'mean_ratio': float(ratio.mean()),\n",
    "                        'median_ratio': float(ratio.median()),\n",
    "                        'std_ratio': float(ratio.std()),\n",
    "                        'min_ratio': float(ratio.min()),\n",
    "                        'max_ratio': float(ratio.max())\n",
    "                    })\n",
    "            \n",
    "            if price_ratios:\n",
    "                analysis['price_ratios'] = {\n",
    "                    'mean_ratio': float(np.mean([pr['mean_ratio'] for pr in price_ratios])),\n",
    "                    'median_ratio': float(np.median([pr['median_ratio'] for pr in price_ratios])),\n",
    "                    'extreme_ratios': int(sum(1 for pr in price_ratios if pr['mean_ratio'] > 3 or pr['mean_ratio'] < 0.33)),\n",
    "                    'details': price_ratios\n",
    "                }\n",
    "            \n",
    "            # ============================================================\n",
    "            # 7. TEMPORAL CONSISTENCY CHECK\n",
    "            # ============================================================\n",
    "            # Check if competitor prices have suspicious temporal patterns\n",
    "            temporal_issues = []\n",
    "            for comp in comp_lag1_cols[:5]:  # Check first 5 for speed\n",
    "                comp_series = df_imputed[comp].dropna()\n",
    "                if len(comp_series) > 30:\n",
    "                    # Check for flat periods (same value for many consecutive days)\n",
    "                    consecutive_same = []\n",
    "                    current_run = 1\n",
    "                    for i in range(1, len(comp_series)):\n",
    "                        if comp_series.iloc[i] == comp_series.iloc[i-1]:\n",
    "                            current_run += 1\n",
    "                        else:\n",
    "                            if current_run > 1:\n",
    "                                consecutive_same.append(current_run)\n",
    "                            current_run = 1\n",
    "                    \n",
    "                    if consecutive_same:\n",
    "                        temporal_issues.append({\n",
    "                            'competitor': comp,\n",
    "                            'max_consecutive_identical': int(max(consecutive_same)),\n",
    "                            'mean_consecutive_identical': float(np.mean(consecutive_same)),\n",
    "                            'n_flat_periods': int(len(consecutive_same))\n",
    "                        })\n",
    "            \n",
    "            if temporal_issues:\n",
    "                analysis['temporal_consistency'] = {\n",
    "                    'competitors_with_flat_periods': int(len(temporal_issues)),\n",
    "                    'max_flat_period_length': int(max([ti['max_consecutive_identical'] for ti in temporal_issues])),\n",
    "                    'details': temporal_issues\n",
    "                }\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR analyzing {hotel_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Analyze ALL hotels\n",
    "print(\"\\nAnalyzing all hotels...\")\n",
    "all_analyses = {}\n",
    "\n",
    "for idx, hotel_id in enumerate(ALL_HOTELS, 1):\n",
    "    if idx % 5 == 0:\n",
    "        print(f\"  Progress: {idx}/{len(ALL_HOTELS)} hotels...\")\n",
    "    \n",
    "    analysis = analyze_imputation_quality(hotel_id)\n",
    "    if analysis:\n",
    "        all_analyses[hotel_id] = analysis\n",
    "\n",
    "print(f\"\\nCompleted analysis for {len(all_analyses)} hotels\")\n",
    "\n",
    "# Save detailed results\n",
    "with open(output_path / 'imputation_analysis_complete.json', 'w') as f:\n",
    "    json.dump(all_analyses, f, indent=2)\n",
    "\n",
    "# ============================================================\n",
    "# GENERATE COMPREHENSIVE SUMMARY REPORT\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE IMPUTATION QUALITY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary statistics\n",
    "summary_stats = []\n",
    "for hotel_id, analysis in all_analyses.items():\n",
    "    stats = {\n",
    "        'hotel_id': hotel_id,\n",
    "        'n_observations': analysis['n_observations'],\n",
    "        'n_competitors': analysis['n_competitors'],\n",
    "        'focal_price_mean': analysis['focal_price_stats']['mean'],\n",
    "        'focal_price_cv': analysis['focal_price_stats']['cv'],\n",
    "        'focal_negative_count': analysis['focal_price_stats']['n_negative'],\n",
    "        'focal_zero_count': analysis['focal_price_stats']['n_zero'],\n",
    "        'competitor_missing_pct': analysis.get('competitor_all_features', {}).get('missing_pct', 0),\n",
    "        'mean_correlation': analysis.get('correlations', {}).get('mean_corr', None),\n",
    "        'weak_correlations': analysis.get('correlations', {}).get('n_weak_corr', None),\n",
    "        'max_identical_pct': analysis.get('imputation_artifacts', {}).get('max_pct_identical', 0),\n",
    "        'high_duplication_count': analysis.get('imputation_artifacts', {}).get('competitors_high_duplication_30', 0),\n",
    "        'mean_uniqueness': analysis.get('imputation_artifacts', {}).get('mean_uniqueness_pct', None)\n",
    "    }\n",
    "    summary_stats.append(stats)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(output_path / 'imputation_summary.csv', index=False)\n",
    "\n",
    "# ============================================================\n",
    "# 1. DATA QUALITY ISSUES\n",
    "# ============================================================\n",
    "print(\"\\n1. FOCAL HOTEL PRICE ISSUES:\")\n",
    "print(\"-\" * 40)\n",
    "issues_found = False\n",
    "for hotel_id, analysis in all_analyses.items():\n",
    "    focal = analysis['focal_price_stats']\n",
    "    if focal['n_negative'] > 0 or focal['n_zero'] > 5 or focal['min'] < 0:\n",
    "        issues_found = True\n",
    "        print(f\"{hotel_id}:\")\n",
    "        print(f\"  Negative: {focal['n_negative']}, Zero: {focal['n_zero']}\")\n",
    "        print(f\"  Price range: ${focal['min']:.2f} - ${focal['max']:.2f}\")\n",
    "        print(f\"  Mean: ${focal['mean']:.2f}, CV: {focal['cv']:.3f}\")\n",
    "if not issues_found:\n",
    "    print(\"  ✓ No major issues found\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. COMPETITOR PRICE ISSUES\n",
    "# ============================================================\n",
    "print(\"\\n2. COMPETITOR PRICE ISSUES:\")\n",
    "print(\"-\" * 40)\n",
    "issues_found = False\n",
    "for hotel_id, analysis in all_analyses.items():\n",
    "    if 'competitor_price_stats_all' in analysis:\n",
    "        comp = analysis['competitor_price_stats_all']\n",
    "        if comp['n_negative'] > 0 or comp['pct_zero'] > 5:\n",
    "            issues_found = True\n",
    "            print(f\"{hotel_id}:\")\n",
    "            print(f\"  Negative: {comp['n_negative']} ({comp['pct_negative']:.1f}%)\")\n",
    "            print(f\"  Zero: {comp['n_zero']} ({comp['pct_zero']:.1f}%)\")\n",
    "            print(f\"  Min: ${comp['min']:.2f}, Max: ${comp['max']:.2f}\")\n",
    "if not issues_found:\n",
    "    print(\"  ✓ No major issues found\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. HIGH MISSING DATA AFTER IMPUTATION\n",
    "# ============================================================\n",
    "print(\"\\n3. REMAINING MISSING DATA (>10% after imputation):\")\n",
    "print(\"-\" * 40)\n",
    "high_missing = summary_df[summary_df['competitor_missing_pct'] > 10].sort_values('competitor_missing_pct', ascending=False)\n",
    "if len(high_missing) > 0:\n",
    "    for _, row in high_missing.iterrows():\n",
    "        print(f\"{row['hotel_id']}: {row['competitor_missing_pct']:.1f}% missing\")\n",
    "else:\n",
    "    print(\"  ✓ All hotels have <10% missing data\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. WEAK CORRELATIONS\n",
    "# ============================================================\n",
    "print(\"\\n4. WEAK COMPETITOR CORRELATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "weak_corr = summary_df[summary_df['mean_correlation'].notna() & (summary_df['mean_correlation'] < 0.3)].sort_values('mean_correlation')\n",
    "if len(weak_corr) > 0:\n",
    "    for _, row in weak_corr.iterrows():\n",
    "        print(f\"{row['hotel_id']}: Mean corr = {row['mean_correlation']:.3f}, Weak = {int(row['weak_correlations'])}/{int(row['n_competitors'])}\")\n",
    "else:\n",
    "    print(\"  ✓ All hotels have reasonable correlations (>0.3)\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. IMPUTATION ARTIFACTS\n",
    "# ============================================================\n",
    "print(\"\\n5. IMPUTATION ARTIFACTS (High Duplication >30%):\")\n",
    "print(\"-\" * 40)\n",
    "artifacts = summary_df[summary_df['high_duplication_count'] > 0].sort_values('max_identical_pct', ascending=False)\n",
    "if len(artifacts) > 0:\n",
    "    for _, row in artifacts.iterrows():\n",
    "        print(f\"{row['hotel_id']}:\")\n",
    "        print(f\"  Max identical: {row['max_identical_pct']:.1f}%\")\n",
    "        print(f\"  Competitors with >30% duplication: {int(row['high_duplication_count'])}/{int(row['n_competitors'])}\")\n",
    "        print(f\"  Mean uniqueness: {row['mean_uniqueness']:.1f}%\")\n",
    "else:\n",
    "    print(\"  ✓ No significant duplication artifacts detected\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. TEMPORAL CONSISTENCY ISSUES\n",
    "# ============================================================\n",
    "print(\"\\n6. TEMPORAL CONSISTENCY ISSUES:\")\n",
    "print(\"-\" * 40)\n",
    "issues_found = False\n",
    "for hotel_id, analysis in all_analyses.items():\n",
    "    if 'temporal_consistency' in analysis:\n",
    "        temp = analysis['temporal_consistency']\n",
    "        if temp['max_flat_period_length'] > 30:\n",
    "            issues_found = True\n",
    "            print(f\"{hotel_id}:\")\n",
    "            print(f\"  Competitors with flat periods: {temp['competitors_with_flat_periods']}\")\n",
    "            print(f\"  Max consecutive identical values: {temp['max_flat_period_length']} days\")\n",
    "if not issues_found:\n",
    "    print(\"  ✓ No major temporal consistency issues\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. OVERALL SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal hotels analyzed: {len(all_analyses)}\")\n",
    "print(f\"\\nFocal Hotel Prices:\")\n",
    "print(f\"  Hotels with negative prices: {(summary_df['focal_negative_count'] > 0).sum()}\")\n",
    "print(f\"  Hotels with zero prices: {(summary_df['focal_zero_count'] > 5).sum()}\")\n",
    "print(f\"  Mean price CV: {summary_df['focal_price_cv'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nCompetitor Data Quality:\")\n",
    "print(f\"  Hotels with >10% missing data: {(summary_df['competitor_missing_pct'] > 10).sum()}\")\n",
    "print(f\"  Mean missing data: {summary_df['competitor_missing_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nCorrelations:\")\n",
    "print(f\"  Hotels with mean corr <0.3: {(summary_df['mean_correlation'] < 0.3).sum()}\")\n",
    "print(f\"  Overall mean correlation: {summary_df['mean_correlation'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nImputation Artifacts:\")\n",
    "print(f\"  Hotels with >30% duplication: {(summary_df['high_duplication_count'] > 0).sum()}\")\n",
    "print(f\"  Mean max duplication: {summary_df['max_identical_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\n\\nDetailed results saved to: {output_path}\")\n",
    "print(\"Files:\")\n",
    "print(\"  - imputation_analysis_complete.json (full details)\")\n",
    "print(\"  - imputation_summary.csv (summary table)\")\n",
    "print(\"\\nComplete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
